{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "P2NDqRbj3gbq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CS 533 Final Project\n",
        "\n",
        "Sakib Jalal, Tanya Balaraju, Aditya Geria\n",
        "\n",
        "Professor Matthew Stone - Natural Language, Spring 2018\n",
        "\n",
        "## Deep Learning Text Generation\n",
        "\n",
        "Through this project, we aimed to generate text based on a given corpus (in this case, J.K. Rowling's *Harry Potter* series) using an RNN (Recurrent Neural Network) consisting of multiple LSTM layers. The RNN was built using Python/Keras, and training was done on an [AWS g2.8xlarge EC2 instance](https://aws.amazon.com/blogs/aws/new-g2-instance-type-with-4x-more-gpu-power/). \n",
        "\n",
        "Originally, the decision to generate *Harry Potter*-like text was based on the ease of performing qualitative analysis on the outcome--the training data is from a single, popular author with a distinctive style, making it easy to compare the RNN's output to the original text. After this initial analysis, we planned to train the RNN on news articles to generate news and analyze the \"tendencies\" of news articles. However, due to financial limitations (we ran out of AWS credit), this is not possible for the time being.\n",
        "\n",
        "We used [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (Long Short Term Memory) cells in our model because we read about how [promising past results](http://arxiv.org/pdf/1503.04069.pdf) from using these were. We learned how they improved on Vanilla RNNs, which can selectively remember past outputs, by also keeping state vectors that enable selective forgetting. We read about GRU (Gated Recurrent Unit) cells and how they combine the 'forgetting' and 'remembering' into a single 'update' step, but decided to just experiment with LSTMs in this project.\n",
        "\n",
        "The repository that hosts this project is hosted [here](https://github.com/sakib/NLP).\n"
      ]
    },
    {
      "metadata": {
        "id": "boE4-ye3470Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experimental Setup\n",
        "\n",
        "First, we import the necessary libraries from [Keras](https://keras.io)."
      ]
    },
    {
      "metadata": {
        "id": "9qu26a4J3do_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e383f21-1bc7-4e69-a83e-73c4d829dac6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525655033612,
          "user_tz": 240,
          "elapsed": 9067,
          "user": {
            "displayName": "Sakib J",
            "photoUrl": "//lh6.googleusercontent.com/-81kvd9bmoJ4/AAAAAAAAAAI/AAAAAAAAKng/__9mjXskexM/s50-c-k-no/photo.jpg",
            "userId": "105785312768351368455"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.wrappers import TimeDistributed"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "wrw9VpsE9u85",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we define some constants to help us control the architecture of our RNN.\n",
        "\n",
        "The following constants should *not* be modified:\n",
        "- `SEQ_LENGTH` refers to the length of the character window considered when predicting the next character in the generated sequence.\n",
        "- `HIDDEN_DIM` refers to the size of the LSTM layers in the RNN.\n",
        "- `LAYER_NUM` refers to the amount of LSTM layers we want in addition to the initial input LSTM layer.\n",
        "- `BATCH_SIZE` is a level of training granularity: weights inside the RNN are updated after each batch of `BATCH_SIZE` training samples are processed.\n",
        "- `DROPOUT_RATE` controls the level to which we randomly drop inputs to the RNN during training, which helps avoid overfitting to the text.\n",
        "\n",
        "The following constants can be modified:\n",
        "- `GENERATE_LENGTH` refers to the length of the output sequence that we want to see when we run the generative model and output text.\n",
        "- `WEIGHTS` is the file location that we want to initially load the network weights from. It can be left as empty: `''`\n",
        "- `TRAIN` is a flag that tells the program whether we want to be training the RNN on this run."
      ]
    },
    {
      "metadata": {
        "id": "TStuXRsW9vma",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# constants - DO NOT CHANGE\n",
        "SEQ_LENGTH = 100\n",
        "HIDDEN_DIM = 700\n",
        "LAYER_NUM = 2\n",
        "BATCH_SIZE = 50\n",
        "DROPOUT_RATE = 0.3\n",
        "\n",
        "# constants - modifiable\n",
        "GENERATE_LENGTH = 500\n",
        "#WEIGHTS = 'weights/hp/checkpoint_layer_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, 300)\n",
        "WEIGHTS = ''\n",
        "TRAIN = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XhXKvixs46nW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Scheme\n",
        "\n",
        "Then, let's define a routine for preparing the training data. The goal of the generative model is as follows: given a sequence of up to `SEQ_LENGTH` characters, predict the next character in the sequence. Here is the scheme:\n",
        "\n",
        "- We load all of the text into `data` and create a small set of characters `chars` of length `VOCAB_SIZE`.\n",
        "- We create mappings from characters to unique numbers and vice versa.\n",
        "- We use these unique numbers to map individual characters to one-hot vectors of length `VOCAB_SIZE`.\n",
        "- The model will output a vector of length `VOCAB_SIZE`. The index of the softmax is taken and mapped back to a character to continue the output stream."
      ]
    },
    {
      "metadata": {
        "id": "OoF-7Lmy3Tjc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def load_data(data_files, seq_length):\n",
        "    data = '\\n'.join([f.read() for f in data_files])\n",
        "    chars = list(set(data))\n",
        "    VOCAB_SIZE = len(chars)\n",
        "\n",
        "    print('Data length: {} characters'.format(len(data)))\n",
        "    print('Vocabulary size: {} characters'.format(VOCAB_SIZE))\n",
        "\n",
        "    ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
        "    char_to_ix = {char:ix for ix, char in enumerate(chars)}\n",
        "\n",
        "    # one hot vectors input [0:100], one hot vectors output [1:101]\n",
        "    X = np.zeros((int(len(data)/seq_length), seq_length, VOCAB_SIZE))\n",
        "    y = np.zeros((int(len(data)/seq_length), seq_length, VOCAB_SIZE))\n",
        "    for i in range(0, int(len(data)/seq_length)):\n",
        "        X_sequence = data[i*seq_length:(i+1)*seq_length]\n",
        "        X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n",
        "        input_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
        "        for j in range(seq_length):\n",
        "            input_sequence[j][X_sequence_ix[j]] = 1.\n",
        "            X[i] = input_sequence\n",
        "\n",
        "        y_sequence = data[i*seq_length+1:(i+1)*seq_length+1]\n",
        "        y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
        "        target_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
        "        for j in range(seq_length):\n",
        "            target_sequence[j][y_sequence_ix[j]] = 1.\n",
        "            y[i] = target_sequence\n",
        "    return X, y, VOCAB_SIZE, ix_to_char"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCOIsZyk5-I_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we load the data into variables. Note that the third book in the *Harry Potter* series is not included here because no uncorrupted version was found. It was important to make sure that the text data was at least a few `MB` in size, which it was, even without the third book."
      ]
    },
    {
      "metadata": {
        "id": "FWc9eAqf6kpd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "2eb81897-d3ae-403d-b110-169226f0c427",
        "executionInfo": {
          "status": "error",
          "timestamp": 1525655035337,
          "user_tz": 240,
          "elapsed": 462,
          "user": {
            "displayName": "Sakib J",
            "photoUrl": "//lh6.googleusercontent.com/-81kvd9bmoJ4/AAAAAAAAAAI/AAAAAAAAKng/__9mjXskexM/s50-c-k-no/photo.jpg",
            "userId": "105785312768351368455"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# parse the data\n",
        "print('\\nloading data...')\n",
        "files = [open('data/hp/hp{}.txt'.format(i), 'r') for i in range(1, 8, 1) if i != 3]\n",
        "X, y, VOCAB_SIZE, ix_to_char = load_data(files, SEQ_LENGTH)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "loading data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-00a8aa1e3564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nloading data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/hp/hp{}.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix_to_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-00a8aa1e3564>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nloading data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/hp/hp{}.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix_to_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/hp/hp1.txt'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "j75GYpOtBIfD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Architecture\n",
        "\n",
        "- `LAYER_NUM` LSTM layers, each of size `HIDDEN_DIM`, with an input dropout rate of `DROPOUT_RATE`, followed by a fully-connected Dense layer of size `VOCAB_SIZE` applied to every temporal slice of the input sequence length axis (size `SEQ_LENGTH`). \n",
        "- Example: `2` LSTM layers, each of size `700`, with an input dropout rate of `0.3`, followed by a fully-connected Dense layer of size `67` applied to every temporal slice of the input on the input sequence length axis (size `100`).\n",
        "\n",
        "With this architecture, we apply the `softmax` activation function to have the RNN yield the index of the maximum value in the vector of length `VOCAB_SIZE`, indicating the most likely character to follow the input sequence."
      ]
    },
    {
      "metadata": {
        "id": "oew9xZE09Ohh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# build the model, lstm, but can replace with gru or simplernn\n",
        "print('\\nbuilding model...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
        "model.add(Dropout(DROPOUT_RATE))\n",
        "for i in range(LAYER_NUM - 1):\n",
        "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ucCdbOl5HSz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Text Generation\n",
        "\n",
        "Now we just need a routine to generate an output sequence. We seed the model with a random character from the vocabulary, convert it to a one-hot encoding, predict the next character in the text based on the sliding window of up to `SEQ_LENGTH` prior characters, and rinse and repeat. "
      ]
    },
    {
      "metadata": {
        "id": "zIzMffOc5Jcl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# method for generating text\n",
        "def generate_text(model, length, vocab_size, ix_to_char):\n",
        "    # starting with random character\n",
        "    ix = [np.random.randint(vocab_size)]\n",
        "    y_char = [ix_to_char[ix[-1]]]\n",
        "    X = np.zeros((1, length, vocab_size))\n",
        "    for i in range(length):\n",
        "        # appending the last predicted character to sequence\n",
        "        X[0, i, :][ix[-1]] = 1\n",
        "        print(ix_to_char[ix[-1]], end=\"\")\n",
        "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
        "        y_char.append(ix_to_char[ix[-1]])\n",
        "    print()\n",
        "    return ('').join(y_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pG1yzi4d5KJJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, we illustrate pre-training results through a call to the `generate_text` routine in order to output 100 garbage characters. We also track the current epoch we are on and load weights from `WEIGHTS` if it was specified above."
      ]
    },
    {
      "metadata": {
        "id": "IAkZeOrA9PHz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# generate stuff before training to see it being bad\n",
        "print('\\npre-training results...')\n",
        "generate_text(model, 100, VOCAB_SIZE, ix_to_char)\n",
        "\n",
        "if WEIGHTS == '':\n",
        "    epochs = 0\n",
        "else:\n",
        "    epochs = int(WEIGHTS[WEIGHTS.rfind('_') + 1:WEIGHTS.find('.')])\n",
        "    print('\\nloading weights from epoch {}...'.format(epochs))\n",
        "    model.load_weights(WEIGHTS)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mZdKGEWi5LRq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training/Generation\n",
        "\n",
        "If `TRAIN` is specified or `WEIGHTS` is unspecified, we train the model with the `fit` routine and `generate_text` along the way, saving weights every `10` epochs. Each epoch ran for about 27 minutes on the AWS GPU hardware."
      ]
    },
    {
      "metadata": {
        "id": "T6bS6ymj9Rdg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "if TRAIN or WEIGHTS == '':\n",
        "    print('training...')\n",
        "    while True:\n",
        "        print('\\n\\nepoch: {}\\n'.format(epochs))\n",
        "        model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=1)\n",
        "        epochs += 1\n",
        "        print('generating text...')\n",
        "        generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
        "        if epochs % 10 == 0:\n",
        "            print('saving weights to file...')\n",
        "            model.save_weights('weights/hp/checkpoint_layer_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, epochs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q9SxnAva5L_e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If `WEIGHTS` is specified, we generate text only."
      ]
    },
    {
      "metadata": {
        "id": "o5p8wnMG5NB9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Else, loading the trained weights and performing generation only\n",
        "if WEIGHTS != '':\n",
        "    generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3X-dG0zY5Nb9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "Due to the `.hdf5` files containing the weights becoming corrupted for unknown reasons, the network does not successfully load the weights from files. Our progress from running this for 5 days was lost, save for screenshots taken at regular intervals (posted below in the Results section). We are currently re-running the training, and the uncorrupted weight files will be available on Github within a few days after 05/06. Until then, the code will be available in the repository.\n",
        "\n",
        "If you would like to recreate this training, run the following long-running command in a terminal. (We strongly recommend **not** doing this on a personal machine.)\n",
        "\n",
        "```\n",
        "nohup python rnn-harry.py >./hp.log 2>&1 < /dev/null &\n",
        "```\n",
        "\n",
        "You can then `tail -f hp.log` to view output as it is logged.\n",
        "\n",
        "[Here are the screenshots](https://imgur.com/2jPSiIJ) of some of our results during training.\n",
        "\n",
        "From here, it's clear that the generated text is approaching the actual writing style of J.K. Rowling over several epochs, with diminishing returns after ~100 epochs. Characters actually reply to each other, correct punctuation is used, sentences are formed well enough. All of this is pretty astonishing, given that this model is generating this output character-by-character based on probabilities.\n",
        "\n",
        "Also, we were thinking of also using GRUs (Gated Recurrent Units) instead of LSTMs, but we ran out of AWS credit, especially after needing to reserve some for re-training the network and regenerating weights."
      ]
    },
    {
      "metadata": {
        "id": "1Ml7Jx0AYDLt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusion \n",
        "### Exact Matches in Generated Text\n",
        "\n",
        "Here, we perform some analysis using `difflib` to determine whether there are exact matches between the outputted text and the original text."
      ]
    },
    {
      "metadata": {
        "id": "RA5nl6Q0WH1w",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "1ed14b1e-8f63-4939-941d-b3488a404058",
        "executionInfo": {
          "status": "error",
          "timestamp": 1525658737673,
          "user_tz": 240,
          "elapsed": 606,
          "user": {
            "displayName": "Tanya B",
            "photoUrl": "//lh5.googleusercontent.com/-rR8gbYDIw7Y/AAAAAAAAAAI/AAAAAAAAA7Y/E1qwYDkEovc/s50-c-k-no/photo.jpg",
            "userId": "101435684380888020364"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#compare string versions of data\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "\n",
        "all_books = []\n",
        "for i in range(1,8):\n",
        "    with open('data/hp/hp{}.txt'.format(i),'r') as book:\n",
        "        #print (book)\n",
        "        all_books = all_books + (book.readlines())\n",
        "\n",
        "#gen = open('gen.txt', 'r')\n",
        "#gen_s = ''.join(gen.readlines())\n",
        "\n",
        "gen_s = '\"I want to know what he\\'s about to do.\" \\\n",
        "    Harry felt a seat at the back of Kreacher\\'s neck she had chosen to speak. \\\n",
        "    \"Do you know what I think?\" said Harry, staring at the pair of them. \\\n",
        "    \"We won\\'t be sure that you were aware of everything they have seen in the Pensieve.\" \\\n",
        "    The crowd below was sweaty trunks, some looking pearly-white and staring. \\\n",
        "    Harry stared at him in an instant he was pointing at the pair of them, \\\n",
        "    the people in the crowd cheered. They had been excited by the afterpeering theor in the...'\n",
        "\n",
        "#analyze similarities sentence-by-sentence\n",
        "#(comparing the generated text to the full corpus yielded incomplete results)\n",
        "\n",
        "all_books_s = ''.join(all_books)\n",
        "all_books_sents = re.split('.,!?\"()', all_books_s)\n",
        "with open('matches.txt', 'w') as f:\n",
        "    for sent in range(len(all_books_sents)):\n",
        "        sm = SequenceMatcher(None, gen_s, all_books_sents[sent])\n",
        "        for block in sm.get_matching_blocks():\n",
        "            if block[2] > 10:\n",
        "                 f.write('Match: \"{}\" in original \\n \\\n",
        "                         with \"{}\" in generated \\n'.format(all_books_sents[sent][block[1]:block[1]+block[2]], \\\n",
        "                         gen_s[block[0]:block[0]+block[2]]))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-68ad9076d164>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_books\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/hp/hp{}.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m#print (book)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mall_books\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_books\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/hp/hp1.txt'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "u1uH3l-5SYdR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "As the results [(file link)](https://github.com/sakib/NLP/blob/master/matches.txt) of this analysis show, the generated text and the original text have a few exact similarities, but no entire sentences are shared between the samples. This is evidence that the model was not overfitted to the data, although qualitative analysis shows that the generated text is still similar to the original data. (The qualitative similarities can be seen in phrases such as \"said Harry, staring at the pair of them,\" where the \"pair\" is reminiscent to Ron and Hermione from the original text. J.K. Rowling's frequent use of \"said\" as a \"speaking verb\" is also clearly mimicked by the model.)\n",
        "\n",
        "## Significance of this experiment \n",
        "\n",
        "This experiment provides insight into the nature of natural language and the patterns of language in some of its most popular written forms. As this experiment shows, a specific writing style can be mimicked by an algorithm that, at its core, only predicts the most likely character to follow a sequence. Even the writing of J.K. Rowling, one of the most popular creative minds of all time, is defined by key elements that can be reproduced by a neural network. \n",
        "\n",
        "However, this is not to say that human creativity is predictable (at least not for the time being). Evidently, the generated text makes grammatical sense, but it doesn't make holistic sense. To address this issue with text-generative models, researchers are looking into not only keeping states, but also [attention](http://arxiv.org/pdf/1502.03044v2.pdf), which means being able to extract information from some abstract larger collection of information. For example, generating captions for an image might pick sections of the image to consider for every character in the output sequence. Similar ideas may apply to simulating authors' writing styles."
      ]
    }
  ]
}